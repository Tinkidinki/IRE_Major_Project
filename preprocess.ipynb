{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run preprocessing_tweets.ipynb\n",
    "%run preprocessing_reddit.ipynb\n",
    "import json\n",
    "import os \n",
    "import numpy as np\n",
    "import json\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# from preprocessing_tweets import load_dataset,load_true_labels\n",
    "# from preprocessing_reddit import load_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(all_data, whichset ='training'):\n",
    "\n",
    "\n",
    "    tknzr = TweetTokenizer(reduce_len=True)\n",
    "    \n",
    "    # Can join train and dev\n",
    "\n",
    "    if whichset == 'training':\n",
    "        training_set = all_data['train']\n",
    "        \n",
    "    elif (whichset == 'development'):\n",
    "        training_set = all_data['dev']\n",
    "        \n",
    "    elif (whichset == 'testing'):\n",
    "        training_set = all_data['test']\n",
    "        \n",
    "    elif (whichset == 'training+development'):\n",
    "        training_set = all_data['train']+all_data['dev']\n",
    "    \n",
    "    BOW_sents = []\n",
    "    all_extra_feats = []\n",
    "    Y = []\n",
    "    ids = []\n",
    "    for conversation in training_set:\n",
    "        \n",
    "    # work with source tweet\n",
    "        tw = conversation['source']\n",
    "        tw_text = conversation['source']['text']\n",
    "        # Preprocess\n",
    "    # remove stop-words\n",
    "    # remove 'rt' and 'via'\n",
    "    # remove punctuation\n",
    "        words = tknzr.tokenize(tw_text)\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        twitter_stops = ['rt','via', 'RT', 'Via', 'VIA']\n",
    "        words = [w for w in words if not w in twitter_stops]\n",
    "        punct = ['!', '@','Â£','$','%','^','&','*','(',')','<','>','?', ',', '.', '/','{','}','#']\n",
    "        words = [w for w in words if not w in punct]\n",
    "        sent = ''\n",
    "        for w in words:\n",
    "            sent = sent +w+' '\n",
    "        # Extract features    \n",
    "    # add to list to later put in BOW\n",
    "        BOW_sents.append(sent)\n",
    "        ids.append(conversation['id'])\n",
    "    # extract other feats and store them for later\n",
    "    \n",
    "        hashash = 0\n",
    "        hasurl = 0\n",
    "        \n",
    "        if 'entities' in list(tw.keys()):\n",
    "        \n",
    "            if tw['entities']['hashtags']!= []:\n",
    "                hashash = 1\n",
    "            \n",
    "            if tw['entities']['urls']!=[]:\n",
    "                hasurl = 1\n",
    "        \n",
    "        s = 0\n",
    "        d = 0\n",
    "        q = 0\n",
    "        \n",
    "        if whichset == 'testing':\n",
    "            \n",
    "            \n",
    "#             submission_file = '../../rumoureval2019_data/final-eval-key.json' # insert your own predictions here\n",
    "#             submission_full = json.load(open(submission_file, 'r'))\n",
    "#             \n",
    "\n",
    "#             submission_file = 'predictions.json'\n",
    "            with open('final-eval-key.json','r') as f:\n",
    "                submission = json.load(f)\n",
    "            submission = submission['subtaskaenglish']\n",
    "#             print(submission.keys())\n",
    "            if submission[tw['id_str']] == 'support':\n",
    "                s = s+1\n",
    "            elif submission[tw['id_str']] == 'deny':\n",
    "                d = d+1\n",
    "            elif submission[tw['id_str']] == 'query':\n",
    "                q = q+1\n",
    "        \n",
    "            for repl in conversation['replies']:\n",
    "                if submission[repl['id_str']]  == 'support':\n",
    "                    s = s+1\n",
    "                elif submission[repl['id_str']]  == 'deny':\n",
    "                    d = d+1\n",
    "                elif submission[repl['id_str']] == 'query':\n",
    "                    q = q+1\n",
    "        \n",
    "        elif whichset == 'development':\n",
    "            \n",
    "            submission_file = 'stance_answer_dev.json' # insert file with predictions of stance labels for dev set here\n",
    "            submission_full = json.load(open(submission_file, 'r'))\n",
    "            submission = submission_full['subtaskaenglish']\n",
    "            \n",
    "            if submission[tw['id_str']] == 'support':\n",
    "                s = s+1\n",
    "            elif submission[tw['id_str']] == 'deny':\n",
    "                d = d+1\n",
    "            elif submission[tw['id_str']] == 'query':\n",
    "                q = q+1\n",
    "        \n",
    "            for repl in conversation['replies']:\n",
    "                if submission[repl['id_str']]  == 'support':\n",
    "                    s = s+1\n",
    "                elif submission[repl['id_str']]  == 'deny':\n",
    "                    d = d+1\n",
    "                elif submission[repl['id_str']] == 'query':\n",
    "                    q = q+1\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            Y.append(conversation['veracity'])\n",
    "            \n",
    "            tweet_label_dict, _ = load_true_labels()\n",
    "            \n",
    "            stance_labels = tweet_label_dict['train']\n",
    "            \n",
    "            stance_labels.update(tweet_label_dict['dev'])\n",
    "            \n",
    "            if stance_labels[tw['id_str']] == 'support':\n",
    "                s = s+1\n",
    "            elif stance_labels[tw['id_str']] == 'deny':\n",
    "                d = d+1\n",
    "            elif stance_labels[tw['id_str']] == 'query':\n",
    "                q = q+1\n",
    "        \n",
    "            for repl in conversation['replies']:\n",
    "                if stance_labels[repl['id_str']]  == 'support':\n",
    "                    s = s+1\n",
    "                elif stance_labels[repl['id_str']]  == 'deny':\n",
    "                    d = d+1\n",
    "                elif stance_labels[repl['id_str']] == 'query':\n",
    "                    q = q+1\n",
    "            \n",
    "        \n",
    "        ntweets = len(conversation['replies'])+1\n",
    "        support_stanceratio  = float(s)/ntweets          \n",
    "        deny_stanceratio =float(d)/ntweets\n",
    "        question_stanceratio = float(q)/ntweets\n",
    "                                    \n",
    "        extra_feats = [hashash,hasurl,support_stanceratio,deny_stanceratio,question_stanceratio]\n",
    "        \n",
    "        all_extra_feats.append(extra_feats)\n",
    "        \n",
    "    return BOW_sents, all_extra_feats, Y, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "\n",
    "    data = load_dataset()\n",
    "    reddit = load_data()\n",
    "       \n",
    "    data['train'].extend(reddit['train'])\n",
    "    data['dev'].extend(reddit['dev'])\n",
    "    data['test'].extend(reddit['test'])\n",
    "    \n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
