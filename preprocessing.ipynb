{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from copy import deepcopy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the training and development data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_true_labels(dataset_name):\n",
    "\n",
    "    # Training and development datasets should be stored in the downloaded_data folder (see installation instructions).\n",
    "    # The test data is kept in the repo for now.\n",
    "    traindev_path = os.path.join(\"downloaded_data\", \"semeval2019-task8-dataset\", \"traindev\")\n",
    "    data_files = {\"dev\": os.path.join(traindev_path, \"rumoureval-subtaskA-dev.json\"),\n",
    "                  \"train\": os.path.join(traindev_path, \"rumoureval-subtaskA-train.json\"),\n",
    "                  \"test\": \"subtaska.json\"}\n",
    "\n",
    "    # Load the dictionary containing the tweets and labels from the .json file\n",
    "    with open(data_files[dataset_name]) as f:\n",
    "        for line in f:\n",
    "            tweet_label_dict = json.loads(line)\n",
    "\n",
    "    return tweet_label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the data in the form of tweet-reply tree containing 297 source tweets in training data, 56 source tweets in testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "global convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "\n",
    "    # Load labels and split for task A\n",
    "    global convs\n",
    "    dev = load_true_labels(\"dev\")\n",
    "    train = load_true_labels(\"train\")\n",
    "    dev_tweets = dev.keys()\n",
    "    train_tweets = train.keys()\n",
    "\n",
    "    # Load folds and conversations\n",
    "    path_to_folds = os.path.join('downloaded_data', 'semeval2019-task8-dataset/rumoureval-data')\n",
    "    folds = sorted(os.listdir(path_to_folds))\n",
    "    newfolds = [i for i in folds if i[0] != '.']\n",
    "    folds = newfolds\n",
    "    cvfolds = {}\n",
    "    allconv = []\n",
    "    weird_conv = []\n",
    "    weird_struct = []\n",
    "    train_dev_split = {}\n",
    "    train_dev_split['dev'] = []\n",
    "    train_dev_split['train'] = []\n",
    "    train_dev_split['test'] = []\n",
    "    for nfold, fold in enumerate(folds):\n",
    "        path_to_tweets = os.path.join(path_to_folds, fold)\n",
    "        tweet_data = sorted(os.listdir(path_to_tweets))\n",
    "        newfolds = [i for i in tweet_data if i[0] != '.']\n",
    "        tweet_data = newfolds\n",
    "        conversation = {}\n",
    "        coun = 0\n",
    "        for foldr in tweet_data:\n",
    "            flag = 0\n",
    "            conversation['id'] = foldr\n",
    "            path_src = path_to_tweets+'/'+foldr+'/source-tweet'\n",
    "            files_t = sorted(os.listdir(path_src))\n",
    "            with open(os.path.join(path_src, files_t[0])) as f:\n",
    "                    for line in f:\n",
    "                        src = json.loads(line)\n",
    "                        src['used'] = 0\n",
    "                        scrcid = src['id_str']\n",
    "                        # add set and label to tweet info\n",
    "                        # first find the tweet in one of the sets\n",
    "                        # foldr - src tweet id\n",
    "                        if scrcid in dev_tweets:\n",
    "                            src['set'] = 'dev'\n",
    "                            src['label'] = dev[scrcid]\n",
    "                            flag = 'dev'\n",
    "    #                        train_dev_tweets['dev'].append(src)\n",
    "                        elif scrcid in train_tweets:\n",
    "                            src['set'] = 'train'\n",
    "                            src['label'] = train[scrcid]\n",
    "                            flag = 'train'\n",
    "    #                        train_dev_tweets['train'].append(src)\n",
    "                        else:\n",
    "                            print \"Tweet was not found! ID: \", foldr\n",
    "            conversation['source'] = src\n",
    "            if src['text'] is None:\n",
    "                print \"Tweet has no text\", src['id']\n",
    "            tweets = []\n",
    "            path_repl = path_to_tweets+'/'+foldr+'/replies'\n",
    "            files_t = sorted(os.listdir(path_repl))\n",
    "            newfolds = [i for i in files_t if i[0] != '.']\n",
    "            files_t = newfolds\n",
    "            for repl_file in files_t:\n",
    "                with open(os.path.join(path_repl, repl_file)) as f:\n",
    "                    for line in f:\n",
    "                        tw = json.loads(line)\n",
    "                        tw['used'] = 0\n",
    "                        replyid = tw['id_str']\n",
    "                        if replyid in dev_tweets:\n",
    "                            tw['set'] = 'dev'\n",
    "                            tw['label'] = dev[replyid]\n",
    "    #                        train_dev_tweets['dev'].append(tw)\n",
    "                            if flag == 'train':\n",
    "                                print \"The tree is split between sets\", foldr\n",
    "                        elif replyid in train_tweets:\n",
    "                            tw['set'] = 'train'\n",
    "                            tw['label'] = train[replyid]\n",
    "    #                        train_dev_tweets['train'].append(tw)\n",
    "                            if flag == 'dev':\n",
    "                                print \"The tree is split between sets\", foldr\n",
    "                        else:\n",
    "                            print \"Tweet was not found! ID: \", foldr\n",
    "                        tweets.append(tw)\n",
    "                        if tw['text'] is None:\n",
    "                            print \"Tweet has no text\", tw['id']\n",
    "            conversation['replies'] = tweets\n",
    "            path_struct = path_to_tweets+'/'+foldr+'/structure.json'\n",
    "            with open(path_struct) as f:\n",
    "                    for line in f:\n",
    "                        struct = json.loads(line)\n",
    "            if len(struct) > 1:\n",
    "                # print \"Structure has more than one root\"\n",
    "                new_struct = {}\n",
    "                new_struct[foldr] = struct[foldr]\n",
    "                struct = new_struct\n",
    "                weird_conv.append(conversation.copy())\n",
    "                weird_struct.append(struct)\n",
    "                # Take item from structure if key is same as source tweet id\n",
    "            conversation['structure'] = struct\n",
    "            coun += 1\n",
    "            branches = tree2branches(conversation['structure'])\n",
    "            conversation['branches'] = branches\n",
    "            convs = conversation\n",
    "            train_dev_split[flag].append(conversation.copy())\n",
    "            allconv.append(conversation.copy())\n",
    "        cvfolds[fold] = allconv\n",
    "        allconv = []\n",
    "\n",
    "    # Load testing data\n",
    "    path_to_test = os.path.join('downloaded_data', 'semeval2019-task8-test-data')\n",
    "    outer_folders = sorted(os.listdir(path_to_test))\n",
    "    newfolds = [i for i in outer_folders if i[0] != '.']\n",
    "    outer_folders = newfolds\n",
    "\n",
    "    test_folders = []\n",
    "    for out in outer_folders:\n",
    "        path_to_outer = os.path.join('downloaded_data', 'semeval2019-task8-test-data',out)\n",
    "        inner_folders = [os.path.join(out, inner) for inner in sorted(os.listdir(path_to_outer))]\n",
    "        test_folders.extend(inner_folders)\n",
    "\n",
    "    conversation = {}\n",
    "    for tfldr in test_folders:\n",
    "        conversation['id'] = tfldr\n",
    "        path_src = path_to_test+'/'+tfldr+'/source-tweet'\n",
    "        files_t = sorted(os.listdir(path_src))\n",
    "        with open(os.path.join(path_src, files_t[0])) as f:\n",
    "            for line in f:\n",
    "                src = json.loads(line)\n",
    "                src['used'] = 0\n",
    "        conversation['source'] = src\n",
    "        tweets = []\n",
    "        path_repl = path_to_test+'/'+tfldr+'/replies'\n",
    "        files_t = sorted(os.listdir(path_repl))\n",
    "        newfolds = [i for i in files_t if i[0] != '.']\n",
    "        files_t = newfolds\n",
    "        for repl_file in files_t:\n",
    "            with open(os.path.join(path_repl, repl_file)) as f:\n",
    "                for line in f:\n",
    "                    tw = json.loads(line)\n",
    "                    tw['used'] = 0\n",
    "            tweets.append(tw)\n",
    "        conversation['replies'] = tweets\n",
    "        path_struct = path_to_test+'/'+tfldr+'/structure.json'\n",
    "        with open(path_struct) as f:\n",
    "            for line in f:\n",
    "                struct = json.loads(line)\n",
    "        conversation['structure'] = struct\n",
    "        branches = tree2branches(conversation['structure'])\n",
    "        conversation['branches'] = branches\n",
    "        train_dev_split['test'].append(conversation.copy())\n",
    "\n",
    "    return train_dev_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convs['structure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convs['branches']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forming branches from the given tweet structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree2branches(root):\n",
    "    node = root\n",
    "    parent_tracker = []\n",
    "    parent_tracker.append(root)\n",
    "    branch = []\n",
    "    branches = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        node_name = node.keys()[i]\n",
    "        #print node_name\n",
    "        branch.append(node_name)\n",
    "        # get children of the node\n",
    "        first_child = node.values()[i]\n",
    "        # actually all chldren, all tree left under this node\n",
    "        if first_child != []:  # if node has children\n",
    "            node = first_child      # walk down\n",
    "            parent_tracker.append(node)\n",
    "            siblings = first_child.keys()\n",
    "            i = 0  # index of a current node\n",
    "        else:\n",
    "            branches.append(deepcopy(branch))\n",
    "            i = siblings.index(node_name)  # index of a current node\n",
    "            # if the node doesn't have next siblings\n",
    "            while i+1 >= len(siblings):\n",
    "                if node is parent_tracker[0]:  # if it is a root node\n",
    "                    return branches\n",
    "                del parent_tracker[-1]\n",
    "                del branch[-1]\n",
    "                node = parent_tracker[-1]      # walk up ... one step\n",
    "                node_name = branch[-1]\n",
    "                siblings = node.keys()\n",
    "                i = siblings.index(node_name)\n",
    "            i = i+1    # ... walk right\n",
    "#            node =  parent_tracker[-1].values()[i]\n",
    "            del branch[-1]\n",
    "#            branch.append(node.keys()[0])\n",
    "#%%\n",
    "# process tweet into features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### identify the content of url in the tweet text/description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantweet(tweettext, tweet):\n",
    "    #  for hashtag in tweet[\"entities\"][\"hashtags\"]:\n",
    "    #    tweettext = tweettext.replace(hashtag[\"text\"], \"\")\n",
    "    if \"media\" in tweet[\"entities\"]:\n",
    "        for url in tweet[\"entities\"][\"media\"]:\n",
    "            tweettext = tweettext.replace(url[\"url\"], \"picpicpic\")\n",
    "    if \"urls\" in tweet[\"entities\"]:\n",
    "        for url in tweet[\"entities\"][\"urls\"]:\n",
    "            tweettext = tweettext.replace(url[\"url\"], \"urlurlurl\")\n",
    "#  for usermention in tweet[\"entities\"][\"user_mentions\"]:\n",
    "#    tweettext = tweettext.replace(usermention[\"screen_name\"], \"\")\n",
    "    return tweettext\n",
    "# converts sentece to list of tokens/words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing of a single string into BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_wordlist(tweettext, tweet, remove_stopwords=False):\n",
    "\n",
    "    #  Remove non-letters\n",
    "    # NOTE: Is it helpful or not to remove non-letters?\n",
    "    # str_text = re.sub(\"[^a-zA-Z]\",\" \", str_text)\n",
    "    tweettext = cleantweet(tweettext, tweet)\n",
    "    str_text = re.sub(\"[^a-zA-Z]\", \" \", tweettext)\n",
    "    # Convert words to lower case and split them\n",
    "    # words = str_text.lower().split()\n",
    "    words = nltk.word_tokenize(str_text.lower())\n",
    "    # Optionally remove stop words (false by default)\n",
    "    # NOTE: generic list of stop words, should i remove them or not?\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if w not in stops]\n",
    "    # 5. Return a list of words\n",
    "    return(words)\n",
    "#%%\n",
    "# Turn tweet into average of word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Google Word2Vec model to retrieve the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadW2vModel():\n",
    "    # LOAD PRETRAINED MODEL\n",
    "    global model\n",
    "    print (\"Loading the model\")\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            os.path.join('downloaded_data', 'GoogleNews-vectors-negative300.bin'), binary=True)\n",
    "    print (\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector for a tweet by adding the w2v embeddings for all the words in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumw2v(tweet, avg=True):\n",
    "    global model\n",
    "    num_features = 300\n",
    "    temp_rep = np.zeros(num_features)\n",
    "    wordlist = str_to_wordlist(tweet['text'], tweet, remove_stopwords=False)\n",
    "    for w in range(len(wordlist)):\n",
    "        if wordlist[w] in model:\n",
    "            temp_rep += model[wordlist[w]]\n",
    "    if avg:\n",
    "        sumw2v = temp_rep/len(wordlist)\n",
    "    else:\n",
    "        # sum\n",
    "        sumw2v = temp_rep\n",
    "    return sumw2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getW2vCosineSimilarity(words, wordssrc):\n",
    "    global model\n",
    "    words2 = []\n",
    "    for word in words:\n",
    "        if word in model.vocab:  # change to model.wv.vocab\n",
    "            words2.append(word)\n",
    "    wordssrc2 = []\n",
    "    for word in wordssrc:\n",
    "        if word in model.vocab:  # change to model.wv.vocab\n",
    "            wordssrc2.append(word)\n",
    "\n",
    "    if len(words2) > 0 and len(wordssrc2) > 0:\n",
    "        return model.n_similarity(words2, wordssrc2)\n",
    "    return 0.\n",
    "#%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features of the replied tweets. Features such as : similarity with source tweet and previous tweet, no of negative words, presence of links etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet2features(tw, i, branch, conversation):\n",
    "\n",
    "    tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+',\n",
    "                                       '', tw['text'].lower()))\n",
    "    srctweet = conversation['source']['text']\n",
    "    if i > 0:\n",
    "        prevtweet_id = branch[i-1]\n",
    "        if (i-1) == 0:\n",
    "            prevtweet = srctweet\n",
    "        else:\n",
    "            for response in conversation['replies']:\n",
    "                if prevtweet_id == response['id_str']:\n",
    "                    prevtweet = response['text']\n",
    "                    break\n",
    "        srctokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+',\n",
    "                                              '', srctweet.lower()))\n",
    "        prevtokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+',\n",
    "                                               '', prevtweet.lower()))\n",
    "    otherthreadtweets = ''\n",
    "    if i != 0:\n",
    "        otherthreadtweets += srctweet\n",
    "    for response in conversation['replies']:\n",
    "        if response['user']['screen_name'] != tw['user']['screen_name']:\n",
    "            otherthreadtweets += ' ' + response['text']\n",
    "\n",
    "    otherthreadtokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+',\n",
    "                                                  '',\n",
    "                                                  otherthreadtweets.lower()))\n",
    "    features = []\n",
    "    tw['text'] = cleantweet(tw['text'], tw)\n",
    "    issourcetw = int(tw['in_reply_to_screen_name'] == None)\n",
    "    hasqmark = 0\n",
    "    if tw['text'].find('?') >= 0:\n",
    "        hasqmark = 1\n",
    "    hasemark = 0\n",
    "    if tw['text'].find('!') >= 0:\n",
    "        hasemark = 1\n",
    "    hasperiod = 0\n",
    "    if tw['text'].find('.') >= 0:\n",
    "        hasperiod = 0\n",
    "    hasurl = 0\n",
    "    if tw['text'].find('urlurlurl') >= 0 or tw['text'].find('http') >= 0:\n",
    "        hasurl = 1\n",
    "    haspic = 0\n",
    "    if (tw['text'].find('picpicpic') >= 0) or (\n",
    "            tw['text'].find('pic.twitter.com') >= 0) or ( \n",
    "                    tw['text'].find('instagr.am') >= 0):\n",
    "        haspic = 1\n",
    "\n",
    "    hasnegation = 0\n",
    "    negationwords = ['not', 'no', 'nobody', 'nothing', 'none', 'never',\n",
    "                     'neither', 'nor', 'nowhere', 'hardly',\n",
    "                     'scarcely', 'barely', 'don', 'isn', 'wasn',\n",
    "                     'shouldn', 'wouldn', 'couldn', 'doesn']\n",
    "    for negationword in negationwords:\n",
    "        if negationword in tokens:\n",
    "            hasnegation += 1\n",
    "\n",
    "    # Character count using len(text) depends on whether a wide or narrow build of Python was used.\n",
    "    # Let's set the count to be equivalent to that computed on a wide build, i.e. all unicode characters are counted as\n",
    "    # length 1.\n",
    "    tmp_tw_text = tw[\"text\"].encode('raw_unicode_escape')\n",
    "    tmp_tw_text = re.sub(\"(\\\\\\\\U[0-9A-Fa-f]{8})\", \"U\", tmp_tw_text)\n",
    "    tmp_tw_text = re.sub(\"(\\\\\\\\u[0-9A-Fa-f]{4})\", \"U\", tmp_tw_text)\n",
    "    charcount = len(tmp_tw_text)\n",
    "\n",
    "    # To print the character counts before and after this change, along with the tweets themselves, uncomment below\n",
    "    # print len(tw[\"text\"]), \"\\t\", charcount, \\\n",
    "    #       \"\\t\\t\", tw[\"text\"],\\\n",
    "    #       \"\\t\\t\", tw[\"text\"].encode('raw_unicode_escape'), \\\n",
    "    #       \"\\t\\t\", tmp_tw_text\n",
    "\n",
    "    wordcount = len(nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+',\n",
    "                                              '',\n",
    "                                              tw['text'].lower())))\n",
    "\n",
    "    swearwords = []\n",
    "    with open('badwords.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            swearwords.append(line.strip().lower())\n",
    "\n",
    "    hasswearwords = 0\n",
    "    for token in tokens:\n",
    "        if token in swearwords:\n",
    "            hasswearwords += 1\n",
    "    uppers = [l for l in tw['text'] if l.isupper()]\n",
    "    capitalratio = len(uppers)/len(tw['text'])\n",
    "\n",
    "#%%\n",
    "# W2vSimilarity wrt prev, thread, src\n",
    "    if i > 0:\n",
    "        Word2VecSimilarityWrtSource = getW2vCosineSimilarity(tokens, srctokens)\n",
    "        Word2VecSimilarityWrtPrev = getW2vCosineSimilarity(tokens, prevtokens)\n",
    "    else:\n",
    "        Word2VecSimilarityWrtSource = 0\n",
    "        Word2VecSimilarityWrtPrev = 0\n",
    "    Word2VecSimilarityWrtOther = getW2vCosineSimilarity(tokens,\n",
    "                                                        otherthreadtokens)\n",
    "\n",
    "#%%\n",
    "    avgw2v = sumw2v(tw, avg=True)\n",
    "    features = [charcount, wordcount, issourcetw, hasqmark, hasemark,\n",
    "                hasperiod, hasurl, haspic, hasnegation, hasswearwords,\n",
    "                capitalratio, Word2VecSimilarityWrtSource,\n",
    "                Word2VecSimilarityWrtPrev, Word2VecSimilarityWrtOther]\n",
    "    features.extend(avgw2v)\n",
    "    features = np.asarray(features, dtype=np.float32)\n",
    "    return features\n",
    "#%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converts int label to str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertlabel(label):\n",
    "    if label == \"support\":\n",
    "        return(0)\n",
    "    elif label == \"comment\":\n",
    "        return(1)\n",
    "    elif label == \"deny\":\n",
    "        return(2)\n",
    "    elif label == \"query\":\n",
    "        return(3)\n",
    "    else:\n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing the data and saving the numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run load_reddit.ipynb\n",
    "\n",
    "def preprocess_data():\n",
    "    # Create train X, train Y, dev X, dev Y\n",
    "\n",
    "    #%%\n",
    "#     loadW2vModel()\n",
    "    #find max branch length\n",
    "    train_dev_split = load_dataset()\n",
    "    reddit_data = load_data()\n",
    "    print(\"done\")\n",
    "    max_branch_len = {}\n",
    "    max_branch_len['train'] = 0\n",
    "    max_branch_len['dev'] = 0\n",
    "    max_branch_len['test'] = 0\n",
    "\n",
    "    whichset = ['train', 'dev', 'test']\n",
    "    special = []\n",
    "\n",
    "    # first put everything in dict contatining lists for each set\n",
    "    branch_list = {}\n",
    "    branch_list['train'] = []\n",
    "    branch_list['dev'] = []\n",
    "    branch_list['test'] = []\n",
    "    # also store labels\n",
    "\n",
    "    label_list = {}\n",
    "    label_list['train'] = []\n",
    "    label_list['dev'] = []\n",
    "    label_list['test'] = []\n",
    "    # also store IDs\n",
    "\n",
    "    ID_list = {}\n",
    "    ID_list['train'] = []\n",
    "    ID_list['dev'] = []\n",
    "    ID_list['test'] = []\n",
    "\n",
    "    rmdoublemask_list = {}\n",
    "    rmdoublemask_list['train'] = []\n",
    "    rmdoublemask_list['dev'] = []\n",
    "    rmdoublemask_list['test'] = []\n",
    "\n",
    "    dumplabel = {}\n",
    "    dumplabel['train'] = []\n",
    "    dumplabel['dev'] = []\n",
    "    dumplabel['test'] = []\n",
    "\n",
    "    for sset in whichset:\n",
    "        for conversation in train_dev_split[sset]:\n",
    "            all_br_len = []\n",
    "            alltweets = [item for sublist in conversation['branches'] for item in sublist]\n",
    "            uniqtweets = list(np.unique(alltweets))\n",
    "            j = uniqtweets.index(conversation['source']['id_str'])\n",
    "            del uniqtweets[j]   # now uniqtweets are replies only\n",
    "            allrepliesfromfoldr = []\n",
    "            for item in conversation['replies']:\n",
    "                allrepliesfromfoldr.append(item['id_str'])\n",
    "            if allrepliesfromfoldr != uniqtweets:\n",
    "                # print \"No correspondence between structure and replies\"\n",
    "                # print conversation['id']\n",
    "                special.append(conversation['id'])\n",
    "\n",
    "            for branch in conversation['branches']:\n",
    "                branch_rep = []  # list of all tweets in the branch\n",
    "                temp_rmd = []\n",
    "                temp_label = []\n",
    "                temp_id = []\n",
    "                all_br_len.append(len(branch))\n",
    "                for i, tweetid in enumerate(branch):\n",
    "                    # find tweet instance\n",
    "                    if i == 0:\n",
    "                        tweet = conversation['source']\n",
    "                    else:\n",
    "                        # tweet = {}\n",
    "                        for response in conversation['replies']:\n",
    "                            if tweetid == response['id_str']:\n",
    "                                tweet = response\n",
    "                                break\n",
    "                    if sset != 'test':\n",
    "                        label = tweet['label']\n",
    "                        temp_label.append(convertlabel(label))  # convertlabel\n",
    "\n",
    "                    temp_id.append(tweet['id_str'])\n",
    "                    if tweet['used']:\n",
    "                        # if tweet has been processed then take the representation\n",
    "                        representation = tweet['representation']\n",
    "                        temp_rmd.append(0)\n",
    "                    else:\n",
    "                        # if tweet is new then\n",
    "                        # get tweet's representation\n",
    "                        representation = tweet2features(tweet, i,\n",
    "                                                        branch, conversation)\n",
    "                        tweet['representation'] = representation\n",
    "                        tweet['used'] = 1\n",
    "                        temp_rmd.append(1)\n",
    "                    branch_rep.append(representation)\n",
    "                branch_list[sset].append(branch_rep)\n",
    "                rmdoublemask_list[sset].append(temp_rmd)\n",
    "                ID_list[sset].append(temp_id)\n",
    "                if sset != 'test':\n",
    "                    label_list[sset].append(temp_label)\n",
    "            if max(all_br_len) > max_branch_len[sset]:\n",
    "                max_branch_len[sset] = max(all_br_len)\n",
    "    #%%\n",
    "    # after that  transform those lists in numpy array,\n",
    "    # get masks needed and saveto files\n",
    "\n",
    "    branch_arrays = {}\n",
    "    num_features = 314\n",
    "    coun = 0\n",
    "    for sset in whichset:\n",
    "        path_to_saved_data = 'saved_data'\n",
    "        path_to_save_sets = os.path.join(path_to_saved_data, sset)\n",
    "        if not os.path.exists(path_to_save_sets):\n",
    "            os.makedirs(path_to_save_sets)\n",
    "        temp_list = []\n",
    "        mask_list = []\n",
    "        padlabel = []\n",
    "        rmdoublemask = []\n",
    "        ids = []\n",
    "        for j, branch in enumerate(branch_list[sset]):\n",
    "            # first put all tweets in branch to the temp array\n",
    "            temp = np.zeros((max_branch_len[sset], num_features),\n",
    "                            dtype=np.float32)\n",
    "            temp_mask = np.zeros((max_branch_len[sset]), dtype=np.int32)\n",
    "            temp_padlabel = np.zeros((max_branch_len[sset]), dtype=np.int32)\n",
    "            temp_rmdoublemask = np.zeros((max_branch_len[sset]), dtype=np.int32)\n",
    "            temp_ids = np.zeros((max_branch_len[sset]))\n",
    "            temp_ids = [str(a) for a in temp_ids]\n",
    "            for i, tweet in enumerate(branch):\n",
    "                temp[i] = tweet\n",
    "                temp_mask[i] = 1\n",
    "                temp_rmdoublemask[i] = rmdoublemask_list[sset][j][i]\n",
    "                temp_ids[i] = ID_list[sset][j][i]\n",
    "                if sset != 'test':\n",
    "                    temp_padlabel[i] = label_list[sset][j][i]\n",
    "            temp_list.append(temp)\n",
    "            mask_list.append(temp_mask)\n",
    "            rmdoublemask.append(temp_rmdoublemask)\n",
    "            ids.extend(temp_ids)\n",
    "            if sset != 'test':\n",
    "                padlabel.append(temp_padlabel)\n",
    "        branch_arrays[sset] = np.asarray(temp_list)\n",
    "        mask = np.asarray(mask_list)\n",
    "        rmdoublemask = np.asarray(rmdoublemask)\n",
    "        if sset != 'test':\n",
    "            padlabel = np.asarray(padlabel)\n",
    "        # save to files\n",
    "#         if coun == 0:\n",
    "#             print(rmdoublemask)\n",
    "#             print(\"------------------------\")\n",
    "#             print(mask)\n",
    "#             print(\"---------------------------\")\n",
    "#             print(len(branch_arrays[sset][0][0]))\n",
    "            \n",
    "#             print(branch_arrays[sset])\n",
    "        coun += 1\n",
    "        np.save(os.path.join(path_to_save_sets, 'rmdoublemask'), rmdoublemask)\n",
    "        np.save(os.path.join(path_to_save_sets, 'mask'), mask)\n",
    "        np.save(os.path.join(path_to_save_sets, 'branch_arrays'),\n",
    "                branch_arrays[sset])\n",
    "        with open(os.path.join(path_to_save_sets, 'ids.pkl'), 'wb') as f:\n",
    "            pickle.dump(ids, f)\n",
    "        if sset != 'test':\n",
    "            np.save(os.path.join(path_to_save_sets, 'padlabel'), padlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ....\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import NLTK data\n",
    "nltk_data_location = os.path.dirname('./')\n",
    "nltk.download('punkt', download_dir=nltk_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data, preprocess it and store in the saved_data folder\n",
    "preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
