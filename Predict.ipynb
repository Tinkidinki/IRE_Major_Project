{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Contains the following functions: \n",
    "   \n",
    "   eval_train_model - re-trains model on train+dev set and \n",
    "                      evaluates on test set\n",
    "\"\"\"\n",
    "import numpy\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import os\n",
    "import pickle\n",
    "from hyperopt import STATUS_OK\n",
    "from training import build_nn,iterate_minibatches\n",
    "#theano.config.floatX = 'float32'\n",
    "#theano.config.warn_float64 = 'raise'\n",
    "#%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model and predict the label on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_train_model(params):\n",
    "    print (\"Retrain model on train+dev set and evaluate on testing set\")\n",
    "    # Initialise parameters \n",
    "    num_lstm_units = int(params['num_lstm_units'])\n",
    "    num_lstm_layers = int(params['num_lstm_layers'])\n",
    "    num_dense_layers = int(params['num_dense_layers'])\n",
    "    num_dense_units = int(params['num_dense_units'])\n",
    "    num_epochs = params['num_epochs']\n",
    "    learn_rate = params['learn_rate']\n",
    "    mb_size = params['mb_size']\n",
    "    l2reg = params['l2reg']\n",
    "    rng_seed = params['rng_seed']\n",
    "#%%\n",
    "    # Load data\n",
    "    path = 'saved_data'\n",
    "    brancharray = numpy.load(os.path.join(path, 'train/branch_arrays.npy'))\n",
    "    num_features = numpy.shape(brancharray)[-1]\n",
    "    train_mask = numpy.load(os.path.join(path,\n",
    "                                         'train/mask.npy')).astype(numpy.int16)\n",
    "    train_label = numpy.load(os.path.join(path, 'train/padlabel.npy'))\n",
    "    \n",
    "    train_rmdoublemask = numpy.load(\n",
    "                            os.path.join(\n",
    "                                path,\n",
    "                                'train/rmdoublemask.npy')).astype(numpy.int16)\n",
    "    train_rmdoublemask = train_rmdoublemask.flatten()\n",
    "#%%\n",
    "    numpy.random.seed(rng_seed)\n",
    "    rng_inst = numpy.random.RandomState(rng_seed)\n",
    "    lasagne.random.set_rng(rng_inst)\n",
    "    input_var = T.ftensor3('inputs')\n",
    "    mask = T.wmatrix('mask')\n",
    "    target_var = T.ivector('targets')\n",
    "    rmdoublesmask = T.wvector('rmdoublemask')\n",
    "    # Build network\n",
    "    network = build_nn(input_var, mask, num_features,\n",
    "                       num_lstm_layers=num_lstm_layers,\n",
    "                       num_lstm_units=num_lstm_units,\n",
    "                       num_dense_layers=num_dense_layers,\n",
    "                       num_dense_units=num_dense_units)\n",
    "    # This function returns the values of the parameters of all\n",
    "    # layers below one or more given Layer instances,\n",
    "    # including the layer(s) itself.\n",
    "\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss*rmdoublesmask\n",
    "    loss = lasagne.objectives.aggregate(loss, mask.flatten())\n",
    "    # regularisation\n",
    "    l2_penalty = l2reg * lasagne.regularization.regularize_network_params(\n",
    "                                network,\n",
    "                                lasagne.regularization.l2)\n",
    "    loss = loss + l2_penalty\n",
    "\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step.\n",
    "    parameters = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    my_updates = lasagne.updates.adam(loss, parameters,\n",
    "                                      learning_rate=learn_rate)\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function(inputs=[input_var, mask,\n",
    "                                       rmdoublesmask, target_var],\n",
    "                               outputs=loss,\n",
    "                               updates=my_updates,\n",
    "                               on_unused_input='warn')\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, mask], test_prediction,\n",
    "                             on_unused_input='warn')\n",
    "#%%\n",
    "    # READ THE DATA\n",
    "    dev_brancharray = numpy.load(os.path.join(path, 'dev/branch_arrays.npy'))\n",
    "    dev_mask = numpy.load(\n",
    "               os.path.join(\n",
    "                   path,\n",
    "                   'dev/mask.npy')).astype(numpy.int16)\n",
    "    dev_label = numpy.load(os.path.join(path, 'dev/padlabel.npy'))\n",
    "\n",
    "    dev_rmdoublemask = numpy.load(\n",
    "                       os.path.join(\n",
    "                        path,\n",
    "                        'dev/rmdoublemask.npy')).astype(numpy.int16).flatten()\n",
    "\n",
    "    with open(os.path.join(path,'dev/ids.pkl'), 'rb') as handle:\n",
    "        dev_ids_padarray = pickle.load(handle)\n",
    "    \n",
    "    test_brancharray = numpy.load(os.path.join(path, 'test/branch_arrays.npy'))\n",
    "    test_mask = numpy.load(\n",
    "                os.path.join(\n",
    "                    path,\n",
    "                    'test/mask.npy')).astype(numpy.int16)\n",
    "\n",
    "    test_rmdoublemask = numpy.load(\n",
    "                os.path.join(path,\n",
    "                             'test/rmdoublemask.npy')).astype(\n",
    "                                                       numpy.int16).flatten()\n",
    "                \n",
    "    with open(os.path.join(path,'test/ids.pkl'), 'rb') as handle:\n",
    "        test_ids_padarray = pickle.load(handle)\n",
    "\n",
    "#%%\n",
    "    #start training loop\n",
    "    # We iterate over epochs:``\n",
    "    # print \"num_epochs\" + num_epochs\n",
    "    # exit(0)\n",
    "    for epoch in range(num_epochs):\n",
    "        print \"Epochs: \" + str(epoch) + \"/\" + str(num_epochs)\n",
    "        #print(\"Epoch {} \".format(epoch))\n",
    "        train_err = 0\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        for batch in iterate_minibatches(brancharray, train_mask,\n",
    "                                         train_rmdoublemask,\n",
    "                                         train_label, mb_size,\n",
    "                                         max_seq_len=25, shuffle=False):\n",
    "                inputs, mask, rmdmask, targets = batch\n",
    "                train_err += train_fn(inputs, mask,\n",
    "                                      rmdmask, targets)\n",
    "        for batch in iterate_minibatches(dev_brancharray, dev_mask,\n",
    "                                         dev_rmdoublemask,\n",
    "                                         dev_label, mb_size,\n",
    "                                         max_seq_len=20, shuffle=False):\n",
    "                inputs, mask, rmdmask, targets = batch\n",
    "                train_err += train_fn(inputs, mask,\n",
    "                                      rmdmask, targets)\n",
    "    # And a full pass over the test data:\n",
    "    test_ypred = val_fn(test_brancharray, test_mask)\n",
    "    # get class label instead of probabilities\n",
    "    new_test_ypred = numpy.argmax(test_ypred, axis=1).astype(numpy.int32)\n",
    "\n",
    "    #Take mask into account\n",
    "    acv_prediction = numpy.asarray(new_test_ypred)\n",
    "    acv_mask = test_mask.flatten()\n",
    "    clip_dev_ids = [o for o, m in zip(test_ids_padarray, acv_mask) if m == 1]\n",
    "    clip_dev_prediction = [o for o, m in zip(acv_prediction, acv_mask)\n",
    "                           if m == 1]\n",
    "    # remove repeating instances\n",
    "    uniqtwid, uindices2 = numpy.unique(clip_dev_ids, return_index=True)\n",
    "    uniq_dev_prediction = [clip_dev_prediction[i] for i in uindices2]\n",
    "    uniq_dev_id = [clip_dev_ids[i] for i in uindices2]\n",
    "    output = {\n",
    "              'status': STATUS_OK,\n",
    "              'Params': params,\n",
    "              'attachments': {'Predictions': pickle.dumps(uniq_dev_prediction),\n",
    "                              'ID': pickle.dumps(uniq_dev_id)}\n",
    "              }\n",
    "\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
