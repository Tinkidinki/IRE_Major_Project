{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "from outer import convertlabeltostr\n",
    "from preprocessing import load_dataset, load_true_labels\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_depth_pred_true():\n",
    "\n",
    "    # Read the predictions of the model\n",
    "    submission_file = os.path.join(\"output\", \"predictions.txt\")\n",
    "    submission = json.load(open(submission_file, 'r'))\n",
    "\n",
    "    # And then the corresponding test data\n",
    "    test_truevals = load_true_labels(\"test\")\n",
    "\n",
    "    # Load the full dataset and get the list of test tweets and their properties\n",
    "    train_dev_split = load_dataset()\n",
    "    alltestinfo = train_dev_split['test']\n",
    "\n",
    "    alltestbranches = []\n",
    "    # get all test branches out of it\n",
    "    for indx, element in enumerate(alltestinfo):\n",
    "        alltestbranches.extend(element['branches'])\n",
    "    # loop over each tweet in testing set and find its depth to create id: depth dictionary\n",
    "    depthinfo = {}\n",
    "    for tweetid in submission.keys():\n",
    "        for branch in alltestbranches:\n",
    "            if tweetid in branch:\n",
    "                depthinfo[tweetid] = branch.index(tweetid)\n",
    "\n",
    "    return depthinfo, submission, test_truevals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_and_predicted_classes(true_tweet_classes, predicted_tweet_classes):\n",
    "\n",
    "    # Sometimes this function may be called with empty predicted classes (e.g. if trials is not available).\n",
    "    # In that case, return None for the true and predicted lists.\n",
    "    if predicted_tweet_classes is None:\n",
    "        true = None\n",
    "        pred = None\n",
    "        return true, pred\n",
    "\n",
    "    true = []\n",
    "    pred = []\n",
    "\n",
    "    # Generate lists of true and predicted classes for all tweets in this set\n",
    "    for k in true_tweet_classes.keys():\n",
    "        true.append(true_tweet_classes[k])\n",
    "        pred.append(predicted_tweet_classes[k])\n",
    "\n",
    "    return true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_results_at_each_depth(depthinfo, submission, test_truevals):\n",
    "\n",
    "    # Group true labels and predictions according to their depth\n",
    "    # depthinfo id: depth\n",
    "    # submission id: prediction\n",
    "    # test_truvals id: label\n",
    "\n",
    "    depth_groups = {}\n",
    "    depth_groups['0'] = []\n",
    "    depth_groups['1'] = []\n",
    "    depth_groups['2'] = []\n",
    "    depth_groups['3'] = []\n",
    "    depth_groups['4'] = []\n",
    "    depth_groups['5'] = []\n",
    "    depth_groups['6+'] = []\n",
    "\n",
    "\n",
    "    # Find all keys in that depth group\n",
    "    for tweetid, tweetdepth in depthinfo.iteritems():\n",
    "        if tweetdepth == 0:\n",
    "            depth_groups['0'].append(tweetid)\n",
    "        elif tweetdepth == 1:\n",
    "            depth_groups['1'].append(tweetid)\n",
    "        elif tweetdepth == 2:\n",
    "            depth_groups['2'].append(tweetid)\n",
    "        elif tweetdepth == 3:\n",
    "            depth_groups['3'].append(tweetid)\n",
    "        elif tweetdepth == 4:\n",
    "            depth_groups['4'].append(tweetid)\n",
    "        elif tweetdepth == 5:\n",
    "            depth_groups['5'].append(tweetid)\n",
    "        elif tweetdepth >5:\n",
    "            depth_groups['6+'].append(tweetid)\n",
    "\n",
    "    # make a list\n",
    "\n",
    "    depth_predictions = {}\n",
    "    depth_predictions['0'] = []\n",
    "    depth_predictions['1'] = []\n",
    "    depth_predictions['2'] = []\n",
    "    depth_predictions['3'] = []\n",
    "    depth_predictions['4'] = []\n",
    "    depth_predictions['5'] = []\n",
    "    depth_predictions['6+'] = []\n",
    "\n",
    "    depth_labels = {}\n",
    "    depth_labels['0'] = []\n",
    "    depth_labels['1'] = []\n",
    "    depth_labels['2'] = []\n",
    "    depth_labels['3'] = []\n",
    "    depth_labels['4'] = []\n",
    "    depth_labels['5'] = []\n",
    "    depth_labels['6+'] = []\n",
    "\n",
    "    depth_result = {}\n",
    "\n",
    "    for depthgr in depth_groups.keys():\n",
    "        depth_predictions[depthgr] = [submission[x] for x in depth_groups[depthgr]]\n",
    "        depth_labels[depthgr] = [test_truevals[x] for x in depth_groups[depthgr]]\n",
    "\n",
    "        _, _, mactest_F, _ = precision_recall_fscore_support(depth_labels[depthgr],\n",
    "                                                             depth_predictions[depthgr],\n",
    "                                                             average='macro')\n",
    "        _, _, mictest_F, _ = precision_recall_fscore_support(depth_labels[depthgr],\n",
    "                                                             depth_predictions[depthgr],\n",
    "                                                             average='micro')\n",
    "        _, _, test_F, _ = precision_recall_fscore_support(depth_labels[depthgr],\n",
    "                                                          depth_predictions[depthgr])\n",
    "\n",
    "        depth_result[depthgr] = [mactest_F, mictest_F, test_F]\n",
    "\n",
    "    return depth_labels, depth_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table_three(true, pred):\n",
    "\n",
    "#     print \"\\n\\n--- Table 3 ---\"\n",
    "\n",
    "    # Prepare headers for the version of Table 3 from the paper (we'll print some additional details too)\n",
    "    table_three_headers = tuple([\"\", \"Accuracy\", \"Macro-F\"] + sorted(class_labels))\n",
    "    results_headers = (\"Precision\", \"Recall\", \"F-score\", \"Support\")\n",
    "\n",
    "    print \"\\nResults on testing set\"\n",
    "\n",
    "    test_accuracy = accuracy_score(true, pred)\n",
    "    print \"\\nAccuracy =\", test_accuracy\n",
    "\n",
    "    print \"\\nMacro-average:\"\n",
    "    macroavg_prfs = precision_recall_fscore_support(true, pred, average='macro')\n",
    "    for lab, val in zip(results_headers, macroavg_prfs):\n",
    "        if val is not None:\n",
    "            print \"%-12s%-12.3f\" % (lab, val)\n",
    "        else:\n",
    "            print \"%-12s%-12s\" % (lab, \"--\")\n",
    "\n",
    "    print \"\\nPer-class:\"\n",
    "    perclass_prfs = precision_recall_fscore_support(true, pred)\n",
    "    print \"%-12s%-12s%-12s%-12s%-12s\" % tuple([\"\"] + sorted(class_labels))\n",
    "    for lab, vals in zip(results_headers, perclass_prfs):\n",
    "        if lab is \"Support\":\n",
    "            print \"%-12s%-12i%-12i%-12i%-12i\" % (lab, vals[0], vals[1], vals[2], vals[3])\n",
    "        else:\n",
    "            print \"%-12s%-12.3f%-12.3f%-12.3f%-12.3f\" % (lab, vals[0], vals[1], vals[2], vals[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_extra_details(best_trial_id):\n",
    "\n",
    "    trials = pickle.load(open(os.path.join(\"output\", \"trials.txt\"), \"rb\"))\n",
    "\n",
    "    # Print out the best combination of hyperparameters\n",
    "    print \"\\n--- New Table ---\\n\"\n",
    "    print \"The best combination of hyperparameters, found in trial \" + str(best_trial_id) + \", was:\"\n",
    "    for param, param_value in trials.best_trial[\"result\"][\"Params\"].iteritems():\n",
    "        print \"\\t\", param, \"=\", param_value\n",
    "\n",
    "    # Let's examine the loss function at each iteration of the hyperparameter tuning process\n",
    "    print \"\\n--- New Figure ---\"\n",
    "\n",
    "    # Extract the loss values from the full list of results, and calculate the running minimum value\n",
    "    loss = numpy.asarray([r[\"loss\"] for r in trials.results])\n",
    "    running_min_loss = numpy.minimum.accumulate(loss)\n",
    "    lowest_loss = loss[best_trial_id]\n",
    "    all_best_ids = numpy.where(loss == lowest_loss)[0]\n",
    "\n",
    "    # Plot the loss and running loss values against the iteration number, and save to the output folder\n",
    "    plt.plot(range(0, len(loss)), loss, label=\"loss\")\n",
    "    plt.plot(range(0, len(running_min_loss)), running_min_loss, label=\"running min(loss)\")\n",
    "    plt.plot(best_trial_id, lowest_loss, \"ro\", label=\"min(loss)\")\n",
    "    if len(all_best_ids) > 1:\n",
    "        plt.plot(all_best_ids, lowest_loss*numpy.ones(all_best_ids.shape), \"rx\", label=\"repeated min(loss)\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Hyperparameter optimisation\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(os.path.join(\"output\", \"hyperparameter_loss_values.pdf\"))\n",
    "\n",
    "    # Give details of other hyperparameter combinations that also achieved this loss\n",
    "    if len(all_best_ids) > 1:\n",
    "        print \"\\nWARNING: multiple hyperparameter combinations achieved the same lowest loss value as trial\", best_trial_id\n",
    "        print \"ID               \",\n",
    "        for id in all_best_ids:\n",
    "            print \"%-17d\" % id,\n",
    "        print \"\"\n",
    "        for param in trials.results[all_best_ids[0]][\"Params\"]:\n",
    "            print \"%-17s\" % param,\n",
    "            for id in all_best_ids:\n",
    "                print \"%-17.5g\" % trials.results[id][\"Params\"][param],\n",
    "            print \"\"\n",
    "\n",
    "    print \"\\nFigure showing hyperparameter optimisation progress can be found in the output folder.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table_four(depth_labels, depth_result):\n",
    "\n",
    "#     print \"\\n\\n--- Table 4 ---\"\n",
    "    print \"\\nNumber of tweets per depth and performance at each of the depths\\n\"\n",
    "\n",
    "    # Print the column headers\n",
    "    table_four_headers = (\"Depth\", \"# tweets\", \"# Support\", \"# Deny\", \"# Query\", \"# Comment\", \"Accuracy\", \"MacroF\") + class_labels\n",
    "    for col in table_four_headers:\n",
    "        print \"%-11s\" % col,\n",
    "    print \"\"\n",
    "\n",
    "    #  Print results in depth level order\n",
    "    for depth in sorted(depth_result):\n",
    "\n",
    "        # Work out which class the accuracy values refer to (precision_recall_fscore_support() outputs values in the\n",
    "        # sorted order of the unique classes of tweets at that depth)\n",
    "        depth_class_accuracy = depth_result[depth][2]\n",
    "        depth_class_labels = sorted(set(depth_labels[depth]))\n",
    "\n",
    "        # Print the depth and classes of tweets at that depth\n",
    "        print \"%-12s%-11i\" % (depth, len(depth_labels[depth])),\n",
    "        for lab in class_labels:\n",
    "            print \"%-11i\" % depth_labels[depth].count(lab.lower()),\n",
    "\n",
    "        # Print the accuracy, macro-F and class-specific performance at each depth\n",
    "        print \"%-12.3f%-11.3f\" % \\\n",
    "              (depth_result[depth][1], depth_result[depth][0]),\n",
    "        for lab in class_labels:\n",
    "            if lab.lower() in depth_class_labels:\n",
    "                class_ind = depth_class_labels.index(lab.lower())\n",
    "                print \"%-11.3f\" % depth_class_accuracy[class_ind],\n",
    "            else:\n",
    "                print \"%-11.3f\" % 0.0,\n",
    "        print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of tweets per depth and performance at each of the depths\n",
      "\n",
      "Depth       # tweets    # Support   # Deny      # Query     # Comment   Accuracy    MacroF      Support     Deny        Query       Comment     \n",
      "0           56          50          3           3           0           0.893       0.314       0.943       0.000       0.000       0.000       \n",
      "1           1010        10          5           39          956         0.967       0.566       0.182       0.333       0.767       0.983       \n",
      "2           0           0           0           0           0           0.000       nan         0.000       0.000       0.000       0.000       \n",
      "3           0           0           0           0           0           0.000       nan         0.000       0.000       0.000       0.000       \n",
      "4           0           0           0           0           0           0.000       nan         0.000       0.000       0.000       0.000       \n",
      "5           0           0           0           0           0           0.000       nan         0.000       0.000       0.000       0.000       \n",
      "6+          0           0           0           0           0           0.000       nan         0.000       0.000       0.000       0.000       \n",
      "\n",
      "Confusion matrix\n",
      "\n",
      "Lab \\ Pred  Comment     Deny        Query       Support     \n",
      "Comment     942         0           14          0           \n",
      "Deny        4           1           0           3           \n",
      "Query       6           0           33          3           \n",
      "Support     9           0           0           51          \n",
      "\n",
      "Results on testing set\n",
      "\n",
      "Accuracy = 0.963414634146\n",
      "\n",
      "Macro-average:\n",
      "Precision   0.894       \n",
      "Recall      0.687       \n",
      "F-score     0.705       \n",
      "Support     --          \n",
      "\n",
      "Per-class:\n",
      "            Comment     Deny        Query       Support     \n",
      "Precision   0.980       1.000       0.702       0.895       \n",
      "Recall      0.985       0.125       0.786       0.850       \n",
      "F-score     0.983       0.222       0.742       0.872       \n",
      "Support     956         8           42          60          \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omkar/course_material/IRE/major_project/branch-lstm-testing/env/local/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/home/omkar/course_material/IRE/major_project/branch-lstm-testing/env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/omkar/course_material/IRE/major_project/branch-lstm-testing/env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/omkar/course_material/IRE/major_project/branch-lstm-testing/env/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def print_table_five(true, pred):\n",
    "\n",
    "#     print \"\\n\\n--- Table 5 ---\"\n",
    "    print \"\\nConfusion matrix\\n\"\n",
    "\n",
    "    # Generate the confusion matrix and the list of labels (as above, in sorted class order as long as each class\n",
    "    # appears once, which they all do).\n",
    "    conf_mat = confusion_matrix(true, pred)\n",
    "    class_labels_mat = (\"Lab \\\\ Pred\",) + tuple(sorted(class_labels))\n",
    "\n",
    "    # Print the header and then the confusion matrix\n",
    "    print \"%-12s%-12s%-12s%-12s%-12s\" % class_labels_mat\n",
    "    for lab, conf_row in zip(sorted(class_labels), conf_mat):\n",
    "        row = (lab,) + tuple(conf_row)\n",
    "        print \"%-12s%-12i%-12i%-12i%-12i\" % row\n",
    "\n",
    "\n",
    "# First load the full set of tweets.\n",
    "# Then calculate the depth and extract the true and predicted labels for the test set specifically.\n",
    "tweet_depth, test_predicted_labels, test_labels = load_test_depth_pred_true()\n",
    "\n",
    "# If it is present, load data from trials file and format in the same way as the submitted files\n",
    "# (return None if the trials file is not available)\n",
    "dev_labels = load_true_labels(\"dev\")\n",
    "# best_trial, best_loss, dev_predicted_labels = load_trials_data()\n",
    "\n",
    "# Analyse the results separately at each depth\n",
    "level_for_each_depth, results_for_each_depth = \\\n",
    "    calculate_results_at_each_depth(tweet_depth, test_predicted_labels, test_labels)\n",
    "\n",
    "# Get lists of the true and predicted classes for the test and, if possible, development sets\n",
    "true_labels_test, predicted_labels_test = get_true_and_predicted_classes(test_labels, test_predicted_labels)\n",
    "# true_labels_dev, predicted_labels_dev = get_true_and_predicted_classes(dev_labels, dev_predicted_labels)\n",
    "\n",
    "# Define some useful labels for table rows/columns\n",
    "class_labels = (\"Support\", \"Deny\", \"Query\", \"Comment\")\n",
    "\n",
    "# Print the tables\n",
    "print_table_four(level_for_each_depth, results_for_each_depth)\n",
    "print_table_five(true_labels_test, predicted_labels_test)\n",
    "print_table_three(true_labels_test, predicted_labels_test)\n",
    "\n",
    "# If the trials file is available, output more details of the best hyperparameter combinations and prepare a figure\n",
    "# showing the loss during the hyperparameter choice process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
